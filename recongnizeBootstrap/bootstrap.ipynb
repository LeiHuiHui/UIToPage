{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential, model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = '../../newdataset/'\n",
    "\n",
    "# Read a file and return a string\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_data(data_dir):\n",
    "    text = []\n",
    "    images = []\n",
    "    # Load all the files and order them\n",
    "    all_filenames = listdir(data_dir)\n",
    "    all_filenames.sort()\n",
    "    for filename in (all_filenames):\n",
    "        if filename[-3:] == \"npz\":\n",
    "            # Load the images already prepared in arrays\n",
    "            image = np.load(data_dir+filename)\n",
    "            images.append(image['features'])\n",
    "        else:\n",
    "            # Load the boostrap tokens and rap them in a start and end tag\n",
    "            syntax = '<START> ' + load_doc(data_dir+filename) + ' <END>'\n",
    "            # Seperate all the words with a single space\n",
    "            syntax = ' '.join(syntax.split())\n",
    "            # Add a space after each comma\n",
    "            syntax = syntax.replace(',', ' ,')\n",
    "            text.append(syntax)\n",
    "    images = np.array(images, dtype=float)\n",
    "    return images, text\n",
    "\n",
    "train_features, texts = load_data(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the function to create the vocabulary \n",
    "tokenizer = Tokenizer(filters='', split=\" \", lower=False)\n",
    "# Create the vocabulary \n",
    "tokenizer.fit_on_texts([load_doc('resources/bootstrap.vocab')])\n",
    "\n",
    "# Add one spot for the empty word in the vocabulary \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Map the input sentences into the vocabulary indexes\n",
    "train_sequences = tokenizer.texts_to_sequences(texts)\n",
    "# The longest set of boostrap tokens\n",
    "max_sequence = max(len(s) for s in train_sequences)\n",
    "# Specify how many tokens to have in each input sentence\n",
    "max_length = 48\n",
    "\n",
    "def preprocess_data(sequences, features):\n",
    "    X, y, image_data = list(), list(), list()\n",
    "    for img_no, seq in enumerate(sequences):\n",
    "        for i in range(1, len(seq)):\n",
    "            # Add the sentence until the current count(i) and add the current count to the output\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # Pad all the input token sentences to max_sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
    "            # Turn the output into one-hot encoding\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # Add the corresponding image to the boostrap token file\n",
    "            image_data.append(features[img_no])\n",
    "            # Cap the input sentence to 48 tokens and add it\n",
    "            X.append(in_seq[-48:])\n",
    "            y.append(out_seq)\n",
    "    return np.array(X), np.array(y), np.array(image_data)\n",
    "\n",
    "X, y, image_data = preprocess_data(train_sequences, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the encoder\n",
    "image_model = Sequential()\n",
    "image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
    "image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
    "image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "image_model.add(Dense(1024, activation='relu'))\n",
    "image_model.add(Dropout(0.3))\n",
    "\n",
    "image_model.add(RepeatVector(max_length))\n",
    "\n",
    "visual_input = Input(shape=(256, 256, 3,))\n",
    "encoded_image = image_model(visual_input)\n",
    "\n",
    "language_input = Input(shape=(max_length,))\n",
    "language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
    "language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "language_model = LSTM(128, return_sequences=True)(language_model)\n",
    "\n",
    "#Create the decoder\n",
    "decoder = concatenate([encoded_image, language_model])\n",
    "decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
    "optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model for every 2nd epoch\n",
    "filepath=\"org-weights-epoch-{epoch:04d}--val_loss-{val_loss:.4f}--loss-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, period=2)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 624 samples, validate on 70 samples\n",
      "Epoch 1/50\n",
      "624/624 [==============================] - 433s 694ms/step - loss: 2.5188 - val_loss: 2.3914\n",
      "Epoch 2/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 2.3944Epoch 00002: saving model to org-weights-epoch-0002--val_loss-2.3359--loss-2.3942.hdf5\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 2.3942 - val_loss: 2.3359\n",
      "Epoch 3/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 2.3206 - val_loss: 2.2796\n",
      "Epoch 4/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 2.1925Epoch 00004: saving model to org-weights-epoch-0004--val_loss-2.3375--loss-2.1923.hdf5\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 2.1923 - val_loss: 2.3375\n",
      "Epoch 5/50\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 1.9346 - val_loss: 2.9472\n",
      "Epoch 6/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 2.0855Epoch 00006: saving model to org-weights-epoch-0006--val_loss-1.9307--loss-2.0837.hdf5\n",
      "624/624 [==============================] - 431s 690ms/step - loss: 2.0837 - val_loss: 1.9307\n",
      "Epoch 7/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 1.5795 - val_loss: 3.7086\n",
      "Epoch 8/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 1.7952Epoch 00008: saving model to org-weights-epoch-0008--val_loss-3.6979--loss-1.7931.hdf5\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 1.7931 - val_loss: 3.6979\n",
      "Epoch 9/50\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 1.5443 - val_loss: 3.5239\n",
      "Epoch 10/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 1.3020Epoch 00010: saving model to org-weights-epoch-0010--val_loss-2.8830--loss-1.3000.hdf5\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 1.3000 - val_loss: 2.8830\n",
      "Epoch 11/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 1.2939 - val_loss: 3.0323\n",
      "Epoch 12/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 1.1336Epoch 00012: saving model to org-weights-epoch-0012--val_loss-3.0385--loss-1.1318.hdf5\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 1.1318 - val_loss: 3.0385\n",
      "Epoch 13/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 1.0795 - val_loss: 2.6406\n",
      "Epoch 14/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 1.1146Epoch 00014: saving model to org-weights-epoch-0014--val_loss-3.6550--loss-1.1131.hdf5\n",
      "624/624 [==============================] - 431s 690ms/step - loss: 1.1131 - val_loss: 3.6550\n",
      "Epoch 15/50\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 0.9763 - val_loss: 2.6555\n",
      "Epoch 16/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.8404Epoch 00016: saving model to org-weights-epoch-0016--val_loss-3.3881--loss-0.8391.hdf5\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.8391 - val_loss: 3.3881\n",
      "Epoch 17/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.7238 - val_loss: 2.5689\n",
      "Epoch 18/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.6704Epoch 00018: saving model to org-weights-epoch-0018--val_loss-2.5416--loss-0.6693.hdf5\n",
      "624/624 [==============================] - 431s 690ms/step - loss: 0.6693 - val_loss: 2.5416\n",
      "Epoch 19/50\n",
      "624/624 [==============================] - 430s 688ms/step - loss: 0.6502 - val_loss: 2.4529\n",
      "Epoch 20/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.6499Epoch 00020: saving model to org-weights-epoch-0020--val_loss-2.5478--loss-0.6489.hdf5\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 0.6489 - val_loss: 2.5478\n",
      "Epoch 21/50\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 0.6036 - val_loss: 2.2909\n",
      "Epoch 22/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.6855Epoch 00022: saving model to org-weights-epoch-0022--val_loss-2.7605--loss-0.6844.hdf5\n",
      "624/624 [==============================] - 431s 690ms/step - loss: 0.6844 - val_loss: 2.7605\n",
      "Epoch 23/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.6185 - val_loss: 2.4526\n",
      "Epoch 24/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.5774Epoch 00024: saving model to org-weights-epoch-0024--val_loss-1.6272--loss-0.5765.hdf5\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.5765 - val_loss: 1.6272\n",
      "Epoch 25/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.6087 - val_loss: 1.2211\n",
      "Epoch 26/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.5512Epoch 00026: saving model to org-weights-epoch-0026--val_loss-1.4625--loss-0.5503.hdf5\n",
      "624/624 [==============================] - 433s 693ms/step - loss: 0.5503 - val_loss: 1.4625\n",
      "Epoch 27/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.4562 - val_loss: 1.8348\n",
      "Epoch 28/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.4432Epoch 00028: saving model to org-weights-epoch-0028--val_loss-1.9843--loss-0.4425.hdf5\n",
      "624/624 [==============================] - 424s 680ms/step - loss: 0.4425 - val_loss: 1.9843\n",
      "Epoch 29/50\n",
      "624/624 [==============================] - 422s 676ms/step - loss: 0.4239 - val_loss: 1.8936\n",
      "Epoch 30/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.3897Epoch 00030: saving model to org-weights-epoch-0030--val_loss-2.3233--loss-0.3890.hdf5\n",
      "624/624 [==============================] - 420s 674ms/step - loss: 0.3890 - val_loss: 2.3233\n",
      "Epoch 31/50\n",
      "624/624 [==============================] - 429s 687ms/step - loss: 0.3539 - val_loss: 1.9185\n",
      "Epoch 32/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.3667Epoch 00032: saving model to org-weights-epoch-0032--val_loss-2.2568--loss-0.3661.hdf5\n",
      "624/624 [==============================] - 435s 698ms/step - loss: 0.3661 - val_loss: 2.2568\n",
      "Epoch 33/50\n",
      "624/624 [==============================] - 437s 700ms/step - loss: 0.3269 - val_loss: 1.4228\n",
      "Epoch 34/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2993Epoch 00034: saving model to org-weights-epoch-0034--val_loss-2.6568--loss-0.2988.hdf5\n",
      "624/624 [==============================] - 434s 696ms/step - loss: 0.2988 - val_loss: 2.6568\n",
      "Epoch 35/50\n",
      "624/624 [==============================] - 433s 693ms/step - loss: 0.2878 - val_loss: 2.7609\n",
      "Epoch 36/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2461Epoch 00036: saving model to org-weights-epoch-0036--val_loss-1.4849--loss-0.2457.hdf5\n",
      "624/624 [==============================] - 427s 685ms/step - loss: 0.2457 - val_loss: 1.4849\n",
      "Epoch 37/50\n",
      "624/624 [==============================] - 425s 682ms/step - loss: 0.2409 - val_loss: 3.2052\n",
      "Epoch 38/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2841Epoch 00038: saving model to org-weights-epoch-0038--val_loss-2.8192--loss-0.2837.hdf5\n",
      "624/624 [==============================] - 429s 688ms/step - loss: 0.2837 - val_loss: 2.8192\n",
      "Epoch 39/50\n",
      "624/624 [==============================] - 429s 688ms/step - loss: 0.2525 - val_loss: 3.0639\n",
      "Epoch 40/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2262Epoch 00040: saving model to org-weights-epoch-0040--val_loss-2.9372--loss-0.2258.hdf5\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.2258 - val_loss: 2.9372\n",
      "Epoch 41/50\n",
      "624/624 [==============================] - 429s 688ms/step - loss: 0.2426 - val_loss: 2.1543\n",
      "Epoch 42/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2274Epoch 00042: saving model to org-weights-epoch-0042--val_loss-2.3587--loss-0.2270.hdf5\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.2270 - val_loss: 2.3587\n",
      "Epoch 43/50\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 0.2073 - val_loss: 1.9644\n",
      "Epoch 44/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2184Epoch 00044: saving model to org-weights-epoch-0044--val_loss-2.1733--loss-0.2181.hdf5\n",
      "624/624 [==============================] - 432s 692ms/step - loss: 0.2181 - val_loss: 2.1733\n",
      "Epoch 45/50\n",
      "624/624 [==============================] - 432s 693ms/step - loss: 0.1841 - val_loss: 2.0452\n",
      "Epoch 46/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2066Epoch 00046: saving model to org-weights-epoch-0046--val_loss-1.9774--loss-0.2063.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 431s 690ms/step - loss: 0.2063 - val_loss: 1.9774\n",
      "Epoch 47/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.2400 - val_loss: 1.4740\n",
      "Epoch 48/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.2121Epoch 00048: saving model to org-weights-epoch-0048--val_loss-1.7457--loss-0.2117.hdf5\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.2117 - val_loss: 1.7457\n",
      "Epoch 49/50\n",
      "624/624 [==============================] - 430s 689ms/step - loss: 0.1730 - val_loss: 1.9932\n",
      "Epoch 50/50\n",
      "623/624 [============================>.] - ETA: 0s - loss: 0.1498Epoch 00050: saving model to org-weights-epoch-0050--val_loss-2.4342--loss-0.1496.hdf5\n",
      "624/624 [==============================] - 430s 690ms/step - loss: 0.1496 - val_loss: 2.4342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27f74909e8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([image_data, X], y, batch_size=1, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}